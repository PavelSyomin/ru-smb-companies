{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "993ff8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import functools\n",
    "import json\n",
    "import multiprocessing\n",
    "import pathlib\n",
    "import time\n",
    "from typing import List\n",
    "from urllib.parse import urljoin\n",
    "import zipfile\n",
    "\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e87c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Params for preprocessor\n",
    "data_path: str = path to folder with input ZIP archives\n",
    "xsl_path: str = path to a file with xsl stylesheet for transforming original XML files from data_path\n",
    "    to their flattened suitable for Pandas read_xml method views\n",
    "out_path: str = path to a local folder where to store produced CSV file\n",
    "data_source: str = location of data_path ('local' or 'ydisk')\n",
    "clear: bool = remove existing CSV file and folder\n",
    "\"\"\"\n",
    "Config = namedtuple(\n",
    "    \"Config\",\n",
    "    [\"data_path\", \"xsl_path\", \"out_path\", \"data_source\", \"clear\", \"num_workers\", \"chunksize\", \"token\"],\n",
    "    defaults=[\"\", \"\", \"\", \"local\", False, 4, 16, \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae8957c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archive:\n",
    "    def __init__(self, path, start=None, stop=None, step=None):\n",
    "        self._archive = zipfile.ZipFile(path)\n",
    "        self._path = path\n",
    "        self._xml_list = [fn for fn in self._archive.namelist() if \"xml\" in fn][start:stop:step]\n",
    "        self._xml_iterable = iter(self._xml_list)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self._archive.close()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._xml_list)\n",
    "    \n",
    "    def __next__(self):\n",
    "        fn = next(self._xml_iterable)\n",
    "        return fn, self._read(fn)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fn in iter(self._xml_list):\n",
    "            yield fn, self._read(fn)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            fn = self._xml_list[index]\n",
    "            return fn, self._read(fn)\n",
    "        elif isinstance(index, slice):\n",
    "            return Archive(self._path, index.start, index.stop, index.step)\n",
    "        else:\n",
    "            raise IndexError(\"Index for archive must be either int or slice\")            \n",
    "    \n",
    "    def _read(self, fn):\n",
    "        return self._archive.read(fn).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "621e690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(item, xsl_path):\n",
    "    if len(item) != 2:\n",
    "        print(\"make_dataframe function expects filename and its content as a [str, str] tuple\")\n",
    "        return None\n",
    "    \n",
    "    fn, xml_string = item\n",
    "    try:\n",
    "        df = pd.read_xml(xml_string, stylesheet=xsl_path, dtype=str)\n",
    "        df.dropna(how=\"all\", inplace=True)\n",
    "        return df\n",
    "    except ET.XMLSyntaxError as e:\n",
    "        print(f\"XML file {fn} is large, trying to convert it separately using lxml\")\n",
    "        try:\n",
    "            xslt = ET.parse(xsl_path)\n",
    "            transformer = ET.XSLT(xslt)\n",
    "            p = etree.XMLParser(huge_tree=True)\n",
    "            xml = ET.fromstring(xml_string.encode(\"utf8\"), parser=p)\n",
    "            xml_string_converted = str(transformer(xml))\n",
    "            df = pd.read_xml(xml_string_converted, dtype=str)\n",
    "            df.dropna(how=\"all\", inplace=True)\n",
    "            print(\"Converted and loaded successfully\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"Still something is wrong, skipping\")\n",
    "            print(e)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "055f6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    HOST = \"https://cloud-api.yandex.net/v1/\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self._data_path = config.data_path\n",
    "        self._xsl_path = config.xsl_path\n",
    "        self._out_path = config.out_path\n",
    "        self._data_source = config.data_source\n",
    "        self._clear = config.clear\n",
    "        self._num_workers = config.num_workers\n",
    "        self._chunksize = config.chunksize\n",
    "        self._token = config.token\n",
    "        \n",
    "        self._check_config()\n",
    "        \n",
    "        self._make_out_folder()\n",
    "        \n",
    "        self._history_file_path = pathlib.Path(self._out_path) / \"history.json\"\n",
    "        self._history = self._get_history()\n",
    "    \n",
    "    def make_csv(self):\n",
    "        input_files = self._get_files()        \n",
    "        \n",
    "        print(f\"Found {len(input_files)} ZIP archives in data folder\")\n",
    "        \n",
    "        func = functools.partial(make_dataframe, xsl_path=self._xsl_path)        \n",
    "        for filename in input_files:\n",
    "            if filename in self._history:\n",
    "                print(f\"{filename} already processed\")\n",
    "                continue\n",
    "            \n",
    "            path = self._resolve_local_file_path(filename)\n",
    "            print(f\"Processing {path}\")\n",
    "            out_file = pathlib.Path(self._out_path) / f\"{path.stem}.csv\"\n",
    "            \n",
    "            \n",
    "            st = time.time()\n",
    "            archive = Archive(path)\n",
    "            \n",
    "            with multiprocessing.Pool(processes=self._num_workers) as pool:\n",
    "                for df in pool.imap(func, archive, chunksize=self._chunksize):\n",
    "                    if df is None:\n",
    "                        continue\n",
    "                    if out_file.exists():\n",
    "                        df.to_csv(out_file, index=False, header=False, mode=\"a\")\n",
    "                    else:\n",
    "                        df.to_csv(out_file, index=False)\n",
    "            \n",
    "            et = time.time()\n",
    "            duration = et - st\n",
    "            print(f\"Completed in {duration:.2f}s\")\n",
    "            \n",
    "            self._remove_local_file(path)\n",
    "            self._history.append(filename)\n",
    "            self._dump_history()\n",
    "            del archive\n",
    "    \n",
    "    def _check_config(self):\n",
    "        if not pathlib.Path(self._xsl_path).exists():\n",
    "            raise RuntimeError(\"XSL file does not exist\")\n",
    "            \n",
    "        if self._data_source not in (\"local\", \"ydisk\"):\n",
    "            raise ValueError(\"Data source must be either 'local' or 'ydisk'\")\n",
    "    \n",
    "    def _download(self, filename: str) -> str:\n",
    "        print(\"Downloading file from Yandex Disk to /tmp\")\n",
    "        \n",
    "        api_path = \"disk/resources/download\"\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"OAuth {self.TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\",    \n",
    "        }\n",
    "        params = {\n",
    "            \"path\": self._data_path + \"/\" + filename,\n",
    "\n",
    "        }        \n",
    "        url = urljoin(self.HOST, api_path)\n",
    "\n",
    "        resp = requests.get(url, headers=headers, params=params)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Cannot get download URL, see error message below\")\n",
    "            print(resp.json())\n",
    "            return None\n",
    "        \n",
    "        download_url = resp.json().get(\"href\")\n",
    "        resp = requests.get(download_url, headers=headers, stream=True)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Cannot download file\")\n",
    "            return None\n",
    "        \n",
    "        out_file = pathlib.Path(\"/tmp\") / filename\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            for chunk in tqdm(resp.iter_content(2**20)): # chunk size is 1 Mib\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return out_file\n",
    "    \n",
    "    def _dump_history(self):\n",
    "        with open(self._history_file_path, \"w\") as f:\n",
    "            json.dump(self._history, f)\n",
    "    \n",
    "    def _get_files(self) -> List[str]:\n",
    "        if self._data_source == \"local\":\n",
    "            data_folder = pathlib.Path(self._data_path)\n",
    "            files = [f.name for f in data_folder.glob(\"*.zip\")]\n",
    "        else:\n",
    "            files = self._get_files_list_from_ydisk()\n",
    "            \n",
    "        return files\n",
    "    \n",
    "    def _get_files_list_from_ydisk(self) -> List[str]:\n",
    "        print(f\"Getting files list for {self._data_path} on Yandex Disk\")\n",
    "        \n",
    "        result = []\n",
    "        api_path = \"disk/resources\"\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"OAuth {self.TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\",    \n",
    "        }\n",
    "        params = {\n",
    "            \"path\": self._data_path,\n",
    "            \"fields\": \"_embedded.items.path,_embedded.items.type\",\n",
    "            \"limit\": 1000,\n",
    "        }\n",
    "        url = urljoin(self.HOST, api_path)\n",
    "\n",
    "        resp = requests.get(url, headers=headers, params=params)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"Cannot get path medatata, see error message below\")\n",
    "            print(resp.json())\n",
    "            return result\n",
    "\n",
    "        for item in resp.json().get(\"_embedded\", {}).get(\"items\", []):\n",
    "            if item.get(\"type\") == \"file\":\n",
    "                _, _, fn = str(item.get(\"path\")).rpartition(\"/\")\n",
    "                result.append(fn)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _get_history(self):        \n",
    "        if self._history_file_path.exists():\n",
    "            with open(self._history_file_path) as f:\n",
    "                history = json.load(f)\n",
    "        else:\n",
    "            history = []\n",
    "        \n",
    "        return history        \n",
    "    \n",
    "    def _make_out_folder(self):\n",
    "        out_path = pathlib.Path(self._out_path)\n",
    "        \n",
    "        if out_path.exists():\n",
    "            if self._clear:\n",
    "                for f in out_path.iterdir():\n",
    "                    f.unlink()\n",
    "        else:\n",
    "            out_path.mkdir(parents=True)\n",
    "\n",
    "    def _resolve_local_file_path(self, filename: str) -> str:\n",
    "        if self._data_source == \"local\":\n",
    "            file_path = pathlib.Path(self._data_path) / filename\n",
    "        else:\n",
    "            file_path = self._download(filename)\n",
    "        \n",
    "        return file_path\n",
    "    \n",
    "    def _remove_local_file(self, path: str):\n",
    "        if self._data_source == \"local\":\n",
    "            return        \n",
    "        \n",
    "        local_file = pathlib.Path(path)\n",
    "        if not local_file.exists():\n",
    "            return\n",
    "        \n",
    "        local_file.unlink()\n",
    "        print(f\"Local copy of downloaded file at {path} removed\")                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5926ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\"rsmp/data\", \"rsmp.xsl\", \"rsmp_out\", \"local\", False, 3, 8, None)\n",
    "p = Preprocessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a07ef429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 ZIP archives in data folder\n",
      "data-01102019-structure-08012016.zip already processed\n",
      "data-01102020-structure-10102019.zip already processed\n",
      "Processing rsmp/data/data-01112017-structure-08012016.zip\n",
      "XML file VO_RRMSPSV_0000_9965_20170110_000fdfee-cebb-4376-9bd6-3223a07b879b.xml is large, trying to convert it separately using lxml\n",
      "Converted and loaded successfully\n",
      "XML file VO_RRMSPSV_0000_9965_20170110_00e24676-b8b5-463a-9a0a-595d970c4019.xml is large, trying to convert it separately using lxml\n",
      "Converted and loaded successfully\n",
      "XML file VO_RRMSPSV_0000_9965_20170110_01820d2f-500e-402b-9885-406475414626.xml is large, trying to convert it separately using lxml\n",
      "Converted and loaded successfully\n",
      "XML file VO_RRMSPSV_0000_9965_20170110_018e3c77-5ef0-4295-9798-49760a68cb94.xml is large, trying to convert it separately using lxml\n",
      "Converted and loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_146078/3546002894.py\", line 8, in make_dataframe\n",
      "    df = pd.read_xml(xml_string, stylesheet=xsl_path, dtype=str)\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 1118, in read_xml\n",
      "    return _parse(\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 844, in _parse\n",
      "    data_dicts = p.parse_data()\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 564, in parse_data\n",
      "    self.xml_doc = self._parse_doc(self.path_or_buffer)\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 646, in _parse_doc\n",
      "    with preprocess_data(handle_data) as xml_data:\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 734, in preprocess_data\n",
      "    data = io.StringIO(data)\n",
      "KeyboardInterrupt\n",
      "  File \"/tmp/ipykernel_146078/3546002894.py\", line 8, in make_dataframe\n",
      "    df = pd.read_xml(xml_string, stylesheet=xsl_path, dtype=str)\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 1118, in read_xml\n",
      "    return _parse(\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 844, in _parse\n",
      "    data_dicts = p.parse_data()\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 570, in parse_data\n",
      "    elems = self._validate_path()\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 582, in _validate_path\n",
      "    def _validate_path(self) -> list[Any]:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_146078/3546002894.py\", line 8, in make_dataframe\n",
      "    df = pd.read_xml(xml_string, stylesheet=xsl_path, dtype=str)\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 1118, in read_xml\n",
      "    return _parse(\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 844, in _parse\n",
      "    data_dicts = p.parse_data()\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 564, in parse_data\n",
      "    self.xml_doc = self._parse_doc(self.path_or_buffer)\n",
      "  File \"/home/pavel/.venv/hse/lib/python3.10/site-packages/pandas/io/xml.py\", line 655, in _parse_doc\n",
      "    document = fromstring(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 41\u001b[0m, in \u001b[0;36mPreprocessor.make_csv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m archive \u001b[38;5;241m=\u001b[39m Archive(path)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap(func, archive, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunksize):\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:423\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    415\u001b[0m result \u001b[38;5;241m=\u001b[39m IMapIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taskqueue\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m    417\u001b[0m     (\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guarded_task_generation(result\u001b[38;5;241m.\u001b[39m_job,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         result\u001b[38;5;241m.\u001b[39m_set_length\n\u001b[1;32m    422\u001b[0m     ))\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (item \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m chunk)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p.make_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
